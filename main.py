# -*- coding: utf-8 -*-
"""third_lab_Vitya.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nKfrOo-11SD16A8xMPQ8UmJelNzcVxxP
"""

import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.metrics import *

import graphviz

"""# 1. Відкрити та зчитати наданий файл з даними."""

df = pd.read_csv('WQ-R.csv', sep=';')

"""# 2. Визначити та вивести кількість записів та кількість полів у завантаженому наборі даних."""

num_of_rows = len(df)
num_of_columns = len(df.columns)
print('Кількість записів:', num_of_rows)
print('Кількість полів:', num_of_columns)

"""# 3. Вивести перші 10 записів набору даних."""

df.head(10)

"""# 4. Розділити набір даних на навчальну (тренувальну) та тестову вибірки."""

df_train, df_test = train_test_split(df, train_size=0.8, random_state=1)

"""# 5. Використовуючи відповідні функції бібліотеки scikit-learn, збудувати класифікаційну модель дерева прийняття рішень глибини 5 та навчити її на тренувальній вибірці, вважаючи, що в наданому наборі даних цільова характеристика визначається останнім стовпчиком, а всі інші виступають в ролі вихідних аргументів."""

x_train, y_train = df_train.iloc[:, :-1], df_train.iloc[:, -1]
x_test, y_test = df_test.iloc[:, :-1], df_test.iloc[:, -1]

model_g = DecisionTreeClassifier(max_depth=5, criterion='gini')
model_g.fit(x_train, y_train)

model_e = DecisionTreeClassifier(max_depth=5, criterion='entropy')
model_e.fit(x_train, y_train)

"""# 6. Представити графічно побудоване дерево за допомогою бібліотеки graphviz."""

img = export_graphviz(
  model_g,
  feature_names=x_train.columns,
  class_names=list(map(str, y_train.unique())),
  rounded=True,
  filled=True
)
graph = graphviz.Source(img)
graph

img = export_graphviz(
  model_e,
  feature_names=x_train.columns,
  class_names=list(map(str, y_train.unique())),
  rounded=True,
  filled=True
)
graph = graphviz.Source(img)
graph

"""# 7. Обчислити класифікаційні метрики збудованої моделі для тренувальної та тестової вибірки. Представити результати роботи моделі на тестовій вибірці графічно. Порівняти результати, отриманні при застосуванні різних критеріїв розщеплення: інформаційний приріст на основі ентропії чи неоднорідності Джині."""

def metrics(model, x, y):
  predictions = model.predict(x)

  accuracy = accuracy_score(y, predictions)
  precision = precision_score(y, predictions, average='weighted')
  recall = recall_score(y, predictions, average='weighted')
  f_score = f1_score(y, predictions, average='weighted')
  mcc = matthews_corrcoef(y, predictions)
  balanced_acc = balanced_accuracy_score(y, predictions)
  return {
      'Accuracy': accuracy,
      'Precision': precision,
      'Recall': recall,
      'F1-Score': f_score,
      'MCC': mcc,
      'Balanced Accuracy': balanced_acc,
  }

test_g = metrics(model_g, x_test, y_test)
test_e = metrics(model_e, x_test, y_test)

train_g = metrics(model_g, x_train, y_train)
train_e = metrics(model_e, x_train, y_train)

fig = plt.figure(figsize=(9, 10), layout="constrained")
axs = fig.subplots(3, 2, sharex=True, sharey=True)
for i, metric in enumerate(test_g):
  axs[i // 2, i % 2].plot([0, 1], [test_g[metric]]*2, label='ginny')
  axs[i // 2, i % 2].plot([0, 1], [test_e[metric]]*2, label='entropy')
  axs[i // 2, i % 2].set_title(metric)
  axs[i // 2, i % 2].legend()
plt.show()

plt.bar(train_g.keys(), train_g.values())
plt.title('Train ginny metrics values')
plt.show()

plt.bar(train_e.keys(), train_e.values())
plt.title('Train entropy metrics values')
plt.show()

"""# 8. З’ясувати вплив глибини дерева та мінімальної кількості елементів в листі дерева на результати класифікації. Результати представити графічно."""

xs, ys_test, ys_train = [], [], []
for i in range(1, 42):
  model = DecisionTreeClassifier(min_samples_leaf=i, random_state=1)
  model.fit(x_train, y_train)
  xs.append(i)
  ys_train.append(balanced_accuracy_score(y_train, model.predict(x_train)))
  ys_test.append(balanced_accuracy_score(y_test, model.predict(x_test)))

plt.plot(xs, ys_test, label='test')
plt.plot(xs, ys_train, label='train')
plt.legend()
plt.title('Bплив мінімальної кількості елементів в листі дерева на результати класифікації.')
plt.show()

xs, ys_test, ys_train = [], [], []
for i in range(1, 42):
  model = DecisionTreeClassifier(min_samples_leaf=i, random_state=1)
  model.fit(x_train, y_train)
  xs.append(i)
  ys_train.append(balanced_accuracy_score(y_train, model.predict(x_train)))
  ys_test.append(balanced_accuracy_score(y_test, model.predict(x_test)))

plt.plot(xs, ys_test, label='test')
plt.plot(xs, ys_train, label='train')
plt.legend()
plt.title('Bплив максимальної глибини дерева на результати класифікації.')
plt.show()

"""# 9. Навести стовпчикову діаграму важливості атрибутів, які використовувалися для класифікації (див. feature_importances_). Пояснити, яким чином – на Вашу думку – цю важливість можна підрахувати."""

feature_importances = model_g.feature_importances_

feature_names = x_train.columns

sorted_idx = feature_importances.argsort()
plt.barh(range(len(sorted_idx)), feature_importances[sorted_idx], align='center')
plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])
plt.show()

feature_importances = model_e.feature_importances_

feature_names = x_train.columns

sorted_idx = feature_importances.argsort()
plt.barh(range(len(sorted_idx)), feature_importances[sorted_idx], align='center')
plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])
plt.show()

